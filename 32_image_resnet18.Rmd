---
title: "R Notebook"
output:
  word_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

```{r, include=FALSE}
#reticulate::use_python("C:/Users/Liam/AppData/Local/Programs/Python/Python311/python.exe")
```

```{r,eval=TRUE, echo=TRUE,cache=TRUE}
setwd("E:/codes/r/tp12")
train_path <- "./images_train/"
validation_path <- "./images_validation/"
```


```{r, eeval=TRUE, echo=TRUE,cache=TRUE}
library(keras)
#library(reticulate)
#library(tensorflow)
set.seed(123)
```

```{r}
Conv_BN_Relu <- function(filters,kernel_size,strides,input_layer){
  x=layer_conv_2d(object = input_layer, filters = filters,kernel_size = kernel_size,strides = strides,padding = 'same')
  x=layer_batch_normalization(x)
  x=layer_activation(x,activation = 'relu')
  return(x)
}


resiidual_a <- function(input_x,filters) {
  x=Conv_BN_Relu(filters,c(3,3),c(1,1),input_x)
  x=Conv_BN_Relu(filters,c(3,3),c(1,1),x)
  y=layer_add(list(x, input_x))
  return(y)
}

resiidual_b <- function(input_x,filters) {
  x=Conv_BN_Relu(filters,c(3,3),c(2,2),input_x)
  x=Conv_BN_Relu(filters,c(3,3),c(1,1),x)
  input_x=Conv_BN_Relu(filters,c(1,1),c(2,2),input_x)
  y=layer_add(list(x, input_x))
  return(y)
}
```


```{r, eval=TRUE, echo=TRUE,cache=TRUE}
img_width <- 32
img_height <- 32


input.model <- layer_input(shape = c(32, 32, 3))

conv1=Conv_BN_Relu(64,c(7,7),c(1,1),input.model)
conv1_Maxpooling=layer_max_pooling_2d(object = conv1, pool_size =c(3,3), strides=c(2,2), padding='same')

x=resiidual_b(conv1_Maxpooling,64)
x=resiidual_a(x,64)

x=resiidual_b(x,128)
x=resiidual_a(x,128)


x=resiidual_b(x,256)
x=resiidual_a(x,256)
  
x=resiidual_b(x,512)
x=resiidual_a(x,512)

x=layer_global_average_pooling_2d(object = x)
x=layer_flatten(object = x)
x=layer_dense(object = x,units = 3)
x=layer_dropout(object = x, rate = 0.5)
y=layer_activation(x, activation = 'softmax')

image.model <- keras_model(inputs = input.model, outputs = y)


###
summary(image.model)




#opt <- optimizer_adam(learning_rate = 1e-3)
opt <- optimizer_rmsprop(learning_rate = 1e-4, weight_decay = 1e-6)
image.model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = opt,
  metrics = "accuracy"
)
```


```{r, eval=TRUE, echo=TRUE,cache=TRUE}
datagen <- image_data_generator(
  rescale = 1/255,
  #rotation_range=40,
  #width_shift_range=0.2,
  #height_shift_range=0.2,
  #shear_range = 0.2,
  #zoom_range = 0.2,
  horizontal_flip = TRUE,
  #fill_mode='nearest'
)
train_data <- flow_images_from_directory(
  train_path,
  generator = datagen,
  target_size = c(img_width, img_height),
  batch_size = 20,
  class_mode = "categorical"
)
validation_data <- flow_images_from_directory(
  validation_path,
  generator = datagen,
  target_size = c(img_width, img_height),
  batch_size = 20,
  class_mode = "categorical"
)



```
```{r, eval=TRUE, echo=TRUE,cache=TRUE}
history <- image.model %>% fit(
  train_data,
  epochs = 100,
  validation_data = validation_data,
  shuffle = TRUE,
  verbose = 2
)
```
```{r, eval=TRUE, echo=TRUE,cache=TRUE}
plot(history)
save("history",file="32_image_resnet18_ep100_nogen.RData")
save_model_tf(object=image.model, filepath="32_image_resnet18_ep100_nogen_small")
loaded_model <- load_model_tf("32_image_resnet18_ep100_nogen_small")
loaded_model %>% save_model_tf("32_image_resnet18_ep100_nogen_small.keras")
image.model %>% save_model_tf("32_image_resnet18_nogen_ep100.keras")
```

